---
title: "Intro to Time Series Analysis"
author: "Benjamin W Nelson"
date: "12/15/2018"
output: html_document
---

#Note
This includes notes taken from the DataCamp Introduction to Time Series Analysis Course. https://www.datacamp.com/courses/introduction-to-time-series-analysis

###Load Packages
```{r}
library(psych)
```


####This will cover four basic time series models.
1. White Noise (WN)
2. Random Walk (RW)
3. Autogregression (AR)
4. Simple Moving Average (MA)

#Exploratory
* First you should always use print() to see the Start, End, and Frequency of data
* You can also use length() to see the number of observations
* You should also use plot() to look at the data

```{r}
#plot(dataframe, xlab = "title", ylab = "title", type = "b") #note that type = "b" provides both data points and a line connecting each point
```

##Sampling Frequency Functions
* start()- time index of the first observation
* end()- time index of the last observation
* frequency()- number of observations per unit time
* deltat()- fixed time increment between observations
* time()- calculates a vector of time indices, with one element for each time index on which the series was observed
* cycle()- returns the position in the cycle of each observation
* plot()- allows you to view the frequency of the data. This function has methods that automatically incorporate time index information into a figure.
* ts.plot()

###Examples
Use the airpassengers dataset
```{r}
# Plot AirPassengers
plot(AirPassengers)

# View the start and end dates of AirPassengers
start(AirPassengers)
end(AirPassengers)

# Use time(), deltat(), frequency(), and cycle() with AirPassengers 
time(AirPassengers)
deltat(AirPassengers)
frequency(AirPassengers)
cycle(AirPassengers)

```


#Sampling Frequency
1. Exact- time series data is exactly evenly spaced (e.g., hourly observations)
2. Approximate- time serues data that is approximately evenly spaced (e.g., temperature obsdrvations each time you check your email)
3. Missing Values- time series data that has missing values

#Basic Assumptions of Time Series Data
1. Consecutive observations are evenly spaced
2. Apply a discrete-time observation index
3. These rules may only be true approximately (e.g., daily stock prices are only observed on weekdays and may not be available for certain holidays)

#Basic Time Series Objects
* ts()- this creates a time series object. This represents data that is at least approximatley evenly spaced across time. You can specify aspects of the ts() by adding information ts(dataframe, start = year, frequency = 1)
* is.ts()- this checks whether a given object is a time series

###Example 1
```{r}
#Simple Time Series
data_vector <- c(10, 6, 6, 8, 20, 3, 6, 9)

#Turn the data vector into time series
time_series <- ts(data_vector)
#plot time series
plot(time_series)
```

###Example 2
```{r}
#Instead of starting with 1, you can start with a year and then specify the frequency of observations

#Turn the data vector into time series
time_series <- ts(data_vector, start = 2004, frequency = 1)
#plot time series
plot(time_series)
```

###Example 3
```{r}
#See if the data vector and time series are time series
is.ts(data_vector) #false
is.ts(time_series) #true
```

#Time Series Trends
##Types of Trends
1. Linear
2. Rapid Growth
3. Periodic or Sinusodial
4. Variance (i.e., increasing or decreasing variance over time)

##Data Filters to Transform or Remove Different Types of Data Trends
1. log()- this is the natural logarithm transformation. This can linearize a Rapid Growth Trend. This can also stabalize a time series that shows increasing Variance Trend (i.e., variability over time). The one restriction is that it can only be used for positively valued time series data. This will shrink observations greater than one towards zero by small increments and shrink large observations towards zero by greater increments.
2. diff()- this is a difference transformation and it can remove linear trends. This will always have one fewer observation than the original time series data before it was transformed.
3. diff(..., lag = s)- this is for a periodic series. This removes periodic trends. For example if you want to remove seasonal trends (there are four seasons), then you set lag = 4. If you want to remove monthly trends, then you set lag = 12. The default of diff() is lag = 1. Differenced series will have s fewer observations than the original dataset.


# 1. White Noise (WN) Model
###Description:
_Weak white noise_ This is a basic type of time serues model that is the basis for more complex time series models. The processes are fixed with a constant mean and constant variance and do not have a correlation over time. In other words, if the mean is not constant over time, the variance changes over time, or there seems to be a correlation between patterns in the data across time, then it is not White Noise.

The WN Model is ARIMA(0,0,0)

###Simulate WN Model
You can simulate WN model data by using arima.sim, which stands for autoregressive integrated moving average class of models. Broad type of time series model that uses WN model as a case.

* ARIMA(p, d, q)
    + p- the autoregressive order
    + d- the order of integration or differencing
    + q- the moving average order
  
* When all of these are 0, then it is a White Noise Model

```{r}
WN_1 <- arima.sim(model = list(order = c(0, 0, 0)), n = 100)
head(WN_1) #show some data
ts.plot(WN_1) #plot data
describe(WN_1) #The mean should be approx 0 and sd should be approx 1


#You can also add additional arguments to specify the mean and sd
WN_2 <- arima.sim(model = list(order = c(0, 0, 0)), n = 100,
                  mean = 10,
                  sd = 2)
head(WN_2) #show some data
ts.plot(WN_2) #plot data
describe(WN_2) #The mean should be approx 10 and sd should be approx 2

```

###Estimate White Noise Model
```{r}
arima(WN_2, order = c(0, 0, 0)) #You can see that the mean should be approx 10
#Intercept is the estimated mean
# sigma^2 is the estiamted variance. The sd is the square root of this variance estimate.
sqrt(3.84) #This should be the same as the sd

#You can also find the mean and variance separately
mean(WN_2)
var(WN_2)
```


# 2. Random Walk (RW) Model
###Desciption:
_Random Walk Model_ This is an example of a non-stationary process. The RM Model has no specified mean or variance, has strong dependence over time (i.e., each observation is closely related to nearby observations), and changes follow a _White Noise_ pattern that is stable and stationary. Visually, short upwards and downward trends can occur.

The RM Model is recursive white noise data. When you use diff(), then you remove the lng-term trend and end up with a _White Noise_ model

The Random Walk Model is ARIMA(0,1,0). This shows that the order of integration of the model is __1__

The Random Walk Model is defined recursively by

* Tomorrow = Today + Noise
    + Note that Today = Tomorrow - 1
* Y~t~ = Y~t-1~ + e~t~
    + e~t~ is white noise mean zero
* Therefore, Tomorrow - Tomorrow - 1 = diff(Y). In other words, Y~t~ - Y~t-1~ = e~t~

When you simulate RW, this requires an initial point, which is usually 0

You can also have a Random Walk with Drift (i.e., constant. If this is 0 then the time series moves around mean 0. If there is a positive drift coefficient or constant, then there will be an upward trend in the time series. If there is a negative drift coefficient or constant, then there will be a downward trend in the time series).

_ Tomorrow = Constant + Today + Noise
_ Y~t~ = c + Y~t-1~ + e~t~
_ RW with Drift has two parameters. 1) the constant c and 2) the WN variance

##Simulate Data
###Example 1
```{r}
RM_1 <- arima.sim(model = list(order = c(0, 1, 0)), n = 100)
head(RM_1)
ts.plot(RM_1)

#Calculate the first difference in the time series.
diff_RM_1 <- diff(RM_1)
ts.plot(diff_RM_1) #This shows that when you remove the long-term trend of the Random Walk data, then it becomes White Noise
```

###Example 2
You can also add a drift by including an intercept in the Random Walk Model. This corresponds to the slope of the Random Walk time trend. This is done by adding the argument mean =
```{r}
rw_drift_pos <- arima.sim(model = list(order = c(0, 1, 0)), n = 100, mean = 1) #mean = 1, so it will be a positive slope
ts.plot(rw_drift_pos)

rw_drift_neg <- arima.sim(model = list(order = c(0, 1, 0)), n = 100, mean = -1) #mean = -1, so it will be a negative slope
ts.plot(rw_drift_neg)

#You can then also calculate the first difference of the drift

rw_drift_diff_pos <- diff(rw_drift_pos)
ts.plot(rw_drift_diff_pos)

#You can then fit each model
model_pos <- arima(rw_drift_diff_pos)
model_pos

#Save the estimated time trend, which is the intercept
intercept_pos <- model_pos$coef
intercept_pos

#Now that you have the estimated coefficient or time trend, you can add the estimated time trend to the original data
ts.plot(rw_drift_pos) + 
abline(0, intercept_pos)

```

##Stationarity

_ "Stationary processes have distributional stability over time"
_ Time Series may "fluctuate randomly", "but behave similarly from one time point to the next"
_ Departures from stationarity include "time trends, periodicity, and a lack of mean reversion"
    + Note: mean reversion just indicates that the time series tends to move around the mean.

_Types of Stationarity_

1. __Weak Stationary__: The mean, variance, and covariance are constant over time. In other words, the mean, variance, and covariance are not changed by time shifts. For example the covariance between Time 1 and Time 7 should be similar to the covariance between Time 8 and Time 14, because they are both six time units apart

_Purpose of Stationary Models_

1. They can be "modeled with fewer parameters"
2. Don't need a different mean for each observation, rather each observation has a common mean






